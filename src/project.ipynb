{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb497b2e",
   "metadata": {},
   "source": [
    "# CART for Classification -- The Overfitters Group Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704e422",
   "metadata": {},
   "source": [
    "## Part 1. Overview of CART (Classification And Regression Trees)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "CART (Classification And Regression Trees), introduced by Breiman et al. (1984), is a fundamental algorithm in machine learning that constructs binary trees for classification and regression. Unlike algorithms like ID3 or C4.5 which can generate multi-way splits, CART produces strictly binary trees, recursively partitioning the feature space into rectangular regions. This project implements a CART classifier from scratch, designed to mimic the behavior of `sklearn.tree.DecisionTreeClassifier` to ensure correctness and robustness.\n",
    "\n",
    "### Representation\n",
    "\n",
    "The model is represented as a binary tree $T$.\n",
    "- **Nodes**: partition the data based on a splitting rule $x_j \\leq \\tau$.\n",
    "- **Leaves**: assign a class label based on the majority vote of samples falling into that region.\n",
    "\n",
    "For an input vector $\\mathbf{x}$, the prediction function $f(\\mathbf{x})$ traverses the tree from root to leaf:\n",
    "$$ f(\\mathbf{x}) = \\text{argmax}_k \\sum_{i \\in R_{leaf(\\mathbf{x})}} \\mathbb{I}(y_i = k) $$\n",
    "\n",
    "### Loss Function: Gini Impurity\n",
    "\n",
    "We use Gini Impurity as the objective function for splitting, which measures the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node.\n",
    "\n",
    "For a node $t$ with class probabilities $p_k$, the Gini impurity is:\n",
    "$$ G(t) = 1 - \\sum_{k=1}^K p_k^2 $$\n",
    "\n",
    "The quality of a split $s$ (into left child $t_L$ and right child $t_R$) is measured by the **Gini Gain** (or decrease in impurity):\n",
    "$$ \\Delta G(s, t) = G(t) - \\left( \\frac{N_{t_L}}{N_t} G(t_L) + \\frac{N_{t_R}}{N_t} G(t_R) \\right) $$\n",
    "\n",
    "We choose the split $(j^*, \\tau^*)$ that maximizes $\\Delta G$.\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "One significant advantage of CART is interpretability. We quantify the importance of each feature by summing the weighted Gini impurity decrease for all nodes $t$ where feature $j$ is used for splitting:\n",
    "$$ \\text{Importance}(j) = \\sum_{t \\in T: v(t)=j} \\frac{N_t}{N} \\Delta G(s_t, t) $$\n",
    "where $N$ is the total number of samples. These values are typically normalized to sum to 1.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- **Training**: Finding the best split requires sorting feature values. For $N$ samples and $D$ features, the cost at the root is $O(D \\cdot N \\log N)$. Since the tree depth is bounded by $O(\\log N)$ (for balanced trees) or $N$ (worst case), the total training complexity is roughly $O(D \\cdot N^2)$ in the worst case, or $O(D \\cdot N \\log^2 N)$ typically.\n",
    "- **Inference**: $O(\\text{depth})$, which is usually $O(\\log N)$. This makes prediction extremely fast.\n",
    "\n",
    "### Optimizer: Greedy Recursive Partitioning\n",
    "\n",
    "The tree is built using a greedy approach:\n",
    "1. Start with all data at the root.\n",
    "2. Find the best split $(j, \\tau)$ across all features and thresholds.\n",
    "3. Partition data and recurse.\n",
    "4. Stop when max depth is reached, node is pure, or minimum samples constraint is met.\n",
    "\n",
    "### References\n",
    "\n",
    "- Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J., 1984. *Classification and Regression Trees*. Wadsworth, Belmont, CA.\n",
    "- Hastie, T., Tibshirani, R. and Friedman, J., 2009. *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. 2nd ed. Springer, New York.\n",
    "- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E., 2011. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12, pp.2825–2830.\n",
    "- Dua, D. and Graff, C., 2019. UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic). University of California, Irvine. Available at: https://archive.ics.uci.edu/ml (Accessed: 7 December 2025).\n",
    "- Street, W.N., Wolberg, W.H. and Mangasarian, O.L., 1993. Nuclear feature extraction for breast tumor diagnosis. In: *Proceedings of SPIE – Biomedical Image Processing and Biomedical Visualization*, pp.861–870.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1818a7a",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "```\n",
    "Algorithm: CART Tree Construction\n",
    "Input: Dataset D = {(x_i, y_i)}_{i=1}^N, stopping criteria\n",
    "Output: Binary decision tree T\n",
    "\n",
    "function BUILD_TREE(D, depth):\n",
    "    if stopping_criterion(D, depth):\n",
    "        return LEAF_NODE(majority_class(D))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a036a",
   "metadata": {},
   "source": [
    "```\n",
    "    # Find best split\n",
    "    best_gain = 0\n",
    "    best_split = None\n",
    "    \n",
    "    for each feature j in {1, ..., d}:\n",
    "        # Sort unique values of feature j\n",
    "        thresholds = unique_sorted_values(D[:, j])\n",
    "        \n",
    "        for each threshold τ in thresholds:\n",
    "            # Split data\n",
    "            D_left = {(x, y) ∈ D : x_j ≤ τ}\n",
    "            D_right = {(x, y) ∈ D : x_j > τ}\n",
    "            \n",
    "            # Compute Gini gain\n",
    "            gain = Gini(D) - |D_left|/|D| * Gini(D_left) - |D_right|/|D| * Gini(D_right)\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split = (j, τ)\n",
    "    \n",
    "    if best_gain == 0:\n",
    "        return LEAF_NODE(majority_class(D))\n",
    "    \n",
    "    # Create node and recursively build subtrees\n",
    "    node = INTERNAL_NODE(best_split)\n",
    "    node.left = BUILD_TREE(D_left, depth + 1)\n",
    "    node.right = BUILD_TREE(D_right, depth + 1)\n",
    "    \n",
    "    return node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ceed07",
   "metadata": {},
   "source": [
    "**Stopping Criteria:**\n",
    "- Maximum depth reached (`max_depth`)\n",
    "- Minimum samples required to split (`min_samples_split`)\n",
    "- Minimum samples at leaf node (`min_samples_leaf`)\n",
    "- No improvement in Gini gain\n",
    "- All samples belong to the same class (pure node)\n",
    "\n",
    "**Complexity:**\n",
    "- Training: $O(d \\cdot N \\log N \\cdot \\text{depth})$ where $d$ is number of features\n",
    "- Prediction: $O(\\text{depth})$ which is $O(\\log N)$ for balanced trees\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Interpretability**: Tree structure is easy to visualize and understand\n",
    "2. **Non-parametric**: No assumptions about data distribution\n",
    "3. **Handles mixed data**: Works with both numerical and categorical features\n",
    "4. **Feature interactions**: Automatically captures feature interactions\n",
    "5. **Minimal preprocessing**: No need for feature scaling or normalization\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Overfitting**: Tendency to create overly complex trees that don't generalize\n",
    "2. **Instability**: Small changes in data can result in very different trees\n",
    "3. **Bias**: Biased toward features with more levels\n",
    "4. **Local optimum**: Greedy algorithm doesn't guarantee global optimum\n",
    "5. **High variance**: Individual trees have high variance (addressed by ensemble methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44ecbdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb0199",
   "metadata": {},
   "source": [
    "## Part 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4860c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A node in the decision tree structure.\n",
    "    \n",
    "    Each node represents either an internal decision point (split) or a leaf (prediction).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_idx : int, optional\n",
    "        The index of the feature used for splitting at this node.\n",
    "        None for leaf nodes.\n",
    "    threshold : float, optional\n",
    "        The threshold value for the split decision.\n",
    "        Samples with feature_idx <= threshold go left, others go right.\n",
    "        None for leaf nodes.\n",
    "    left : Node, optional\n",
    "        The left child node (feature_idx <= threshold).\n",
    "        None for leaf nodes.\n",
    "    right : Node, optional\n",
    "        The right child node (feature_idx > threshold).\n",
    "        None for leaf nodes.\n",
    "    value : int or float, optional\n",
    "        The predicted class label for this node.\n",
    "        Only set for leaf nodes.\n",
    "    gini : float, optional\n",
    "        The Gini impurity at this node.\n",
    "    n_samples : int, optional\n",
    "        The number of training samples that reached this node.\n",
    "    class_counts : ndarray, optional\n",
    "        Array containing the count of samples for each class at this node.\n",
    "    gain : float, default=0.0\n",
    "        The Gini gain (impurity reduction) achieved by the split at this node.\n",
    "        Used for computing feature importance.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    All parameters become attributes of the Node instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, \n",
    "                 value=None, gini=None, n_samples=None, class_counts=None, gain=0.0):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.gini = gini\n",
    "        self.n_samples = n_samples\n",
    "        self.class_counts = class_counts\n",
    "        self.gain = gain\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Check if this node is a leaf node.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if this is a leaf node (has a prediction value), False otherwise.\n",
    "        \"\"\"\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    A CART (Classification and Regression Tree) Classifier implemented from scratch.\n",
    "    \n",
    "    This implementation uses the Gini impurity as the splitting criterion and builds\n",
    "    a binary decision tree through greedy recursive partitioning. The algorithm\n",
    "    mimics sklearn.tree.DecisionTreeClassifier behavior.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int, optional (default=None)\n",
    "        The maximum depth of the tree. If None, nodes are expanded until\n",
    "        all leaves are pure or contain fewer than min_samples_split samples.\n",
    "    min_samples_split : int, default=2\n",
    "        The minimum number of samples required to split an internal node.\n",
    "        If a node has fewer samples, it becomes a leaf.\n",
    "    min_samples_leaf : int, default=1\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split candidate is rejected if it would create a child with\n",
    "        fewer than min_samples_leaf samples.\n",
    "    random_state : int, optional (default=None)\n",
    "        Controls randomness for reproducibility. Sets the numpy random seed.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    tree_ : Node\n",
    "        The root node of the fitted decision tree.\n",
    "    n_features_ : int\n",
    "        The number of features when fit is performed.\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        The unique class labels found in the training data.\n",
    "    feature_importances_ : ndarray of shape (n_features,)\n",
    "        The normalized feature importances. Computed as the total Gini gain\n",
    "        (weighted by number of samples) contributed by each feature across\n",
    "        all splits in the tree.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> X, y = load_iris(return_X_y=True)\n",
    "    >>> clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "    >>> clf.fit(X, y)\n",
    "    >>> clf.predict(X[:5])\n",
    "    array([0, 0, 0, 0, 0])\n",
    "    >>> clf.score(X, y)\n",
    "    0.98\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The algorithm uses the Gini impurity criterion:\n",
    "        Gini(t) = 1 - sum(p_k^2)\n",
    "    where p_k is the proportion of samples of class k at node t.\n",
    "    \n",
    "    The Gini gain for a split is:\n",
    "        Gain = Gini(parent) - (N_left/N)*Gini(left) - (N_right/N)*Gini(right)\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984).\n",
    "    Classification and Regression Trees. Wadsworth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.tree_ = None\n",
    "        self.n_features_ = None\n",
    "        self.classes_ = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"\n",
    "        Compute Gini impurity for a vector of labels.\n",
    "        \n",
    "        The Gini impurity measures the probability of misclassifying a randomly\n",
    "        chosen element if it were randomly labeled according to the distribution\n",
    "        of labels in the subset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target labels for which to compute Gini impurity.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The Gini impurity value in the range [0, 1-1/K] where K is the\n",
    "            number of classes. Returns 0.0 for empty arrays or pure nodes.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Gini impurity is calculated as:\n",
    "            G(y) = 1 - sum(p_k^2)\n",
    "        where p_k is the proportion of samples belonging to class k.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        return 1.0 - np.sum(probs ** 2)\n",
    "    \n",
    "    def _split_data(self, X, y, feature_idx, threshold):\n",
    "        \"\"\"\n",
    "        Split data into left and right subsets based on a feature threshold.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The feature matrix to split.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target labels to split.\n",
    "        feature_idx : int\n",
    "            The index of the feature to use for splitting.\n",
    "        threshold : float\n",
    "            The threshold value for the split.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_left : ndarray\n",
    "            Feature matrix for samples where X[:, feature_idx] <= threshold.\n",
    "        y_left : ndarray\n",
    "            Labels for samples where X[:, feature_idx] <= threshold.\n",
    "        X_right : ndarray\n",
    "            Feature matrix for samples where X[:, feature_idx] > threshold.\n",
    "        y_right : ndarray\n",
    "            Labels for samples where X[:, feature_idx] > threshold.\n",
    "        \"\"\"\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        return X[left_mask], y[left_mask], X[~left_mask], y[~left_mask]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the decision tree from the training set (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target class labels.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : DecisionTreeClassifier\n",
    "            Returns self to allow method chaining.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        This method builds the tree recursively by:\n",
    "        1. Finding the best split at each node (maximizing Gini gain)\n",
    "        2. Partitioning data based on the split\n",
    "        3. Recursively building left and right subtrees\n",
    "        4. Stopping when termination criteria are met\n",
    "        \n",
    "        Feature importances are accumulated during tree construction and\n",
    "        normalized at the end.\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        # Initialize feature importance tracking\n",
    "        self._raw_feature_importances = np.zeros(self.n_features_)\n",
    "        \n",
    "        self.tree_ = self._build_tree(X, y)\n",
    "        \n",
    "        # Normalize feature importances\n",
    "        total_importance = np.sum(self._raw_feature_importances)\n",
    "        if total_importance > 0:\n",
    "            self.feature_importances_ = self._raw_feature_importances / total_importance\n",
    "        else:\n",
    "            self.feature_importances_ = np.zeros(self.n_features_)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Alias for fit() method.\n",
    "        \n",
    "        Provided for compatibility with certain interfaces that expect a train() method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target class labels.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : DecisionTreeClassifier\n",
    "            Returns self to allow method chaining.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "        \n",
    "        This is the core recursive function that constructs the tree structure\n",
    "        by repeatedly finding the best split and partitioning the data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The feature matrix for samples at this node.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The labels for samples at this node.\n",
    "        depth : int, default=0\n",
    "            The current depth in the tree (root is at depth 0).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Node\n",
    "            The root node of the (sub)tree built from the provided data.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Stopping criteria:\n",
    "        - All samples belong to the same class (pure node)\n",
    "        - Maximum depth reached\n",
    "        - Number of samples less than min_samples_split\n",
    "        - No split improves Gini impurity\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        # Pre-calculate node properties\n",
    "        current_gini = self._gini(y)\n",
    "        counts_vec = np.array([np.sum(y == c) for c in self.classes_])\n",
    "        majority_class = self.classes_[np.argmax(counts_vec)]\n",
    "        \n",
    "        # Create leaf if stopping criteria met\n",
    "        if (n_labels == 1 or \n",
    "            (self.max_depth is not None and depth >= self.max_depth) or \n",
    "            n_samples < self.min_samples_split):\n",
    "            return Node(value=majority_class, gini=current_gini, \n",
    "                       n_samples=n_samples, class_counts=counts_vec)\n",
    "            \n",
    "        best_feat, best_thresh, best_gain = self._find_best_split(X, y, current_gini)\n",
    "        \n",
    "        # If no split improves impurity, return leaf\n",
    "        if best_gain == 0:\n",
    "            return Node(value=majority_class, gini=current_gini, \n",
    "                       n_samples=n_samples, class_counts=counts_vec)\n",
    "        \n",
    "        # Accumulate feature importance: (N_t / N_total) * Gain\n",
    "        self._raw_feature_importances[best_feat] += best_gain * n_samples\n",
    "        \n",
    "        # Execute split using helper\n",
    "        X_left, y_left, X_right, y_right = self._split_data(X, y, best_feat, best_thresh)\n",
    "        \n",
    "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return Node(feature_idx=best_feat, threshold=best_thresh, \n",
    "                   left=left_child, right=right_child, \n",
    "                   gini=current_gini, n_samples=n_samples, \n",
    "                   class_counts=counts_vec, gain=best_gain)\n",
    "\n",
    "    def _find_best_split(self, X, y, parent_gini):\n",
    "        \"\"\"\n",
    "        Find the best split for a node by evaluating all possible splits.\n",
    "        \n",
    "        This method exhaustively searches over all features and all possible\n",
    "        threshold values to find the split that maximizes Gini gain.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The feature matrix.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target labels.\n",
    "        parent_gini : float\n",
    "            The Gini impurity of the parent node (before splitting).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        best_feat : int or None\n",
    "            The index of the best feature to split on, or None if no valid split found.\n",
    "        best_thresh : float or None\n",
    "            The best threshold value for the split, or None if no valid split found.\n",
    "        best_gain : float\n",
    "            The Gini gain achieved by the best split. 0.0 if no valid split found.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        The algorithm:\n",
    "        1. For each feature, extract unique values\n",
    "        2. Test midpoints between consecutive unique values as thresholds\n",
    "        3. For each threshold, compute the weighted Gini impurity of children\n",
    "        4. Select the split with maximum Gini gain\n",
    "        5. Respect min_samples_leaf constraint\n",
    "        \"\"\"\n",
    "        best_gain = 0.0\n",
    "        best_feat = None\n",
    "        best_thresh = None\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for feat_idx in range(self.n_features_):\n",
    "            # Optimization: only check thresholds between unique values\n",
    "            thresholds = np.unique(X[:, feat_idx])\n",
    "            if len(thresholds) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Check midpoints\n",
    "            midpoints = (thresholds[:-1] + thresholds[1:]) / 2\n",
    "            \n",
    "            for thresh in midpoints:\n",
    "                # Optimization: Don't slice X here, just y. But check split size.\n",
    "                left_mask = X[:, feat_idx] <= thresh\n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[~left_mask]\n",
    "                \n",
    "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                n_l, n_r = len(y_left), len(y_right)\n",
    "                gini_l = self._gini(y_left)\n",
    "                gini_r = self._gini(y_right)\n",
    "                \n",
    "                child_gini = (n_l / n_samples) * gini_l + (n_r / n_samples) * gini_r\n",
    "                gain = parent_gini - child_gini\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat = feat_idx\n",
    "                    best_thresh = thresh\n",
    "                    \n",
    "        return best_feat, best_thresh, best_gain\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            The predicted class labels for each sample.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        For each sample, the prediction is made by traversing the tree from\n",
    "        the root to a leaf, following the decision rules at each internal node.\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_one(x, self.tree_) for x in X])\n",
    "    \n",
    "    def _predict_one(self, x, node):\n",
    "        \"\"\"\n",
    "        Predict the class label for a single sample by traversing the tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray of shape (n_features,)\n",
    "            A single input sample.\n",
    "        node : Node\n",
    "            The current node in the tree traversal.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int or float\n",
    "            The predicted class label.\n",
    "        \"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        return self._predict_one(x, node.right)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : ndarray of shape (n_samples, n_classes)\n",
    "            The class probabilities for each sample. Each row sums to 1.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        The probability for each class is computed as the proportion of\n",
    "        training samples of that class in the leaf node where the sample ends up.\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_proba_one(x, self.tree_) for x in X])\n",
    "        \n",
    "    def _predict_proba_one(self, x, node):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for a single sample.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray of shape (n_features,)\n",
    "            A single input sample.\n",
    "        node : Node\n",
    "            The current node in the tree traversal.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_classes,)\n",
    "            The class probabilities for this sample.\n",
    "        \"\"\"\n",
    "        if node.is_leaf():\n",
    "            total = node.class_counts.sum()\n",
    "            return node.class_counts / total if total > 0 else np.zeros(len(self.classes_))\n",
    "        \n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self._predict_proba_one(x, node.left)\n",
    "        return self._predict_proba_one(x, node.right)\n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the misclassification rate on the provided data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The true class labels.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The misclassification rate (proportion of incorrect predictions).\n",
    "        \"\"\"\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds != y)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the accuracy score on the provided data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The true class labels.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy score (proportion of correct predictions).\n",
    "        \"\"\"\n",
    "        return 1.0 - self.loss(X, y)\n",
    "    \n",
    "    def get_depth(self):\n",
    "        \"\"\"\n",
    "        Get the depth of the decision tree.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The maximum depth of the tree. A tree with only a root node has depth 0.\n",
    "        \"\"\"\n",
    "        return self._get_depth_rec(self.tree_)\n",
    "        \n",
    "    def _get_depth_rec(self, node):\n",
    "        \"\"\"\n",
    "        Recursively compute the depth of a subtree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node or None\n",
    "            The root of the subtree.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The depth of the subtree rooted at node.\n",
    "        \"\"\"\n",
    "        if node is None or node.is_leaf():\n",
    "            return 0\n",
    "        return 1 + max(self._get_depth_rec(node.left), self._get_depth_rec(node.right))\n",
    "        \n",
    "    def get_n_leaves(self):\n",
    "        \"\"\"\n",
    "        Get the number of leaf nodes in the decision tree.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of leaf nodes in the tree.\n",
    "        \"\"\"\n",
    "        return self._get_n_leaves_rec(self.tree_)\n",
    "        \n",
    "    def _get_n_leaves_rec(self, node):\n",
    "        \"\"\"\n",
    "        Recursively count the number of leaf nodes in a subtree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node or None\n",
    "            The root of the subtree.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of leaf nodes in the subtree rooted at node.\n",
    "        \"\"\"\n",
    "        if node is None: return 0\n",
    "        if node.is_leaf(): return 1\n",
    "        return self._get_n_leaves_rec(node.left) + self._get_n_leaves_rec(node.right)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Generate a text representation of the tree structure.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A human-readable string representation of the decision tree,\n",
    "            showing the structure with indentation and split/leaf information.\n",
    "        \"\"\"\n",
    "        if self.tree_ is None:\n",
    "            return \"Empty Tree\"\n",
    "        return self._print_tree(self.tree_)\n",
    "        \n",
    "    def _print_tree(self, node, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build a string representation of the tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node\n",
    "            The current node to print.\n",
    "        depth : int, default=0\n",
    "            The current depth (used for indentation).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A formatted string representation of the subtree rooted at node.\n",
    "        \"\"\"\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf():\n",
    "            probs = node.class_counts / node.class_counts.sum()\n",
    "            # Format class probabilities neatly\n",
    "            prob_str = \", \".join([f\"{self.classes_[i]}: {p:.2f}\" for i, p in enumerate(probs)])\n",
    "            return f\"{indent}Leaf: Predict {node.value} (Probs: [{prob_str}], Samples: {node.n_samples})\\n\"\n",
    "        \n",
    "        s = f\"{indent}If Feature {node.feature_idx} <= {node.threshold:.3f} (Gain: {node.gain:.4f})\\n\"\n",
    "        s += self._print_tree(node.left, depth + 1)\n",
    "        s += self._print_tree(node.right, depth + 1)\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8930ac",
   "metadata": {},
   "source": [
    "## Part 3. Check Model\n",
    "\n",
    "We exercise the CART implementation with method-level unit tests (_gini_impurity_, _split_data_, _find_best_split_, depth/leaf constraints, prediction, probability estimates, loss) and edge cases such as single-class data. We then match sklearn on a synthetic separable dataset and the multiclass Iris benchmark, demonstrating functional parity and deterministic behavior. The final block trains on the Breast Cancer Wisconsin data, reports accuracy, confusion matrix, and depth/leaf counts for both our model and sklearn, and sweeps max_depth to show how capacity impacts bias/variance before saving the plot (Breiman et al., 1984; Pedregosa et al., 2011; Dua and Graff, 2019).\n",
    "\n",
    "### Test plan\n",
    "- Unit tests (2–3 per method) cover: gini impurity math; split correctness; best-split search; depth/min-samples constraints; predict memorization on separable data; predict_proba sums/non-negative; score matches manual accuracy; loss equals misclassification.\n",
    "- Edge cases: single-class data, tiny splits/leaves, deterministic behavior via random_state.\n",
    "- Parity demos: our CART matches sklearn on a synthetic separable set and on the multiclass Iris dataset; final experiment reproduces sklearn on Breast Cancer Wisconsin with matching accuracy/confusion matrix and depth/leaf counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "864049c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit tests completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pytest import approx\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDTC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def check_vals(actual, expected, rtol=1e-6, msg=\"\"):\n",
    "    \"\"\"\n",
    "    Assert that two arrays are approximately equal within a relative tolerance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : array-like\n",
    "        The actual values to check.\n",
    "    expected : array-like\n",
    "        The expected values.\n",
    "    rtol : float, default=1e-6\n",
    "        Relative tolerance for comparison.\n",
    "    msg : str, default=\"\"\n",
    "        Error message to display if assertion fails.\n",
    "    \"\"\"\n",
    "    assert np.allclose(actual, expected, rtol=rtol), msg\n",
    "\n",
    "def check_split(X_left, X_right, y_left, y_right, feat_idx, thr):\n",
    "    \"\"\"\n",
    "    Verify that a data split is valid and correctly partitioned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_left, X_right : ndarray\n",
    "        The left and right feature matrices after split.\n",
    "    y_left, y_right : ndarray\n",
    "        The left and right label arrays after split.\n",
    "    feat_idx : int\n",
    "        The feature index used for splitting.\n",
    "    thr : float\n",
    "        The threshold value used for splitting.\n",
    "    \"\"\"\n",
    "    assert X_left.shape[0] + X_right.shape[0] == y_left.size + y_right.size, \"split loses samples\"\n",
    "    assert np.all(X_left[:, feat_idx] <= thr), \"left branch threshold violation\"\n",
    "    assert np.all(X_right[:, feat_idx] > thr), \"right branch threshold violation\"\n",
    "\n",
    "\n",
    "# Test 1: Gini impurity\n",
    "clf = DecisionTreeClassifier()\n",
    "check_vals(clf._gini(np.array([1, 1, 1])), 0.0)\n",
    "check_vals(clf._gini(np.array([0, 1] * 4)), 0.5)\n",
    "check_vals(clf._gini(np.array([0, 0, 1, 1, 2, 2])), 2 / 3)\n",
    "\n",
    "# Test 2: Data split\n",
    "X_s = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y_s = np.array([0, 0, 1, 1])\n",
    "X_l, y_l, X_r, y_r = clf._split_data(X_s, y_s, feature_idx=0, threshold=4)\n",
    "check_split(X_l, X_r, y_l, y_r, feat_idx=0, thr=4)\n",
    "\n",
    "# Test 3: Best split finder (obvious threshold at 3.5)\n",
    "X_simple = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "y_simple = np.array([0, 0, 0, 1, 1, 1])\n",
    "parent_gini = clf._gini(y_simple)\n",
    "clf.n_features_ = X_simple.shape[1]\n",
    "feat, thr, gain = clf._find_best_split(X_simple, y_simple, parent_gini)\n",
    "assert feat == 0\n",
    "assert 3 < thr < 4\n",
    "assert gain > 0.4\n",
    "\n",
    "# Test 4: Depth constraint respected\n",
    "X_depth = np.random.randn(100, 3)\n",
    "y_depth = np.random.randint(0, 2, 100)\n",
    "for d in [1, 3, 5]:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=0).fit(X_depth, y_depth)\n",
    "    assert model.get_depth() <= d\n",
    "\n",
    "# Test 5: min_samples_split makes tree shallower\n",
    "X_mss = np.random.randn(60, 2)\n",
    "y_mss = np.random.randint(0, 2, 60)\n",
    "coarse = DecisionTreeClassifier(min_samples_split=20, random_state=0).fit(X_mss, y_mss)\n",
    "fine = DecisionTreeClassifier(random_state=0).fit(X_mss, y_mss)\n",
    "assert coarse.get_depth() <= fine.get_depth()\n",
    "\n",
    "# Test 6: min_samples_leaf reduces leaves\n",
    "X_msl = np.random.randn(120, 2)\n",
    "y_msl = np.random.randint(0, 2, 120)\n",
    "wide = DecisionTreeClassifier(min_samples_leaf=10, random_state=0).fit(X_msl, y_msl)\n",
    "base = DecisionTreeClassifier(random_state=0).fit(X_msl, y_msl)\n",
    "assert wide.get_n_leaves() <= base.get_n_leaves()\n",
    "\n",
    "# Test 7: Predict memorizes small separable set\n",
    "X_pred = np.array([[1, 1], [5, 5], [2, 2], [6, 6]])\n",
    "y_pred = np.array([0, 1, 0, 1])\n",
    "mem = DecisionTreeClassifier(random_state=0).fit(X_pred, y_pred)\n",
    "check_vals(mem.predict(X_pred), y_pred)\n",
    "\n",
    "# Test 8: predict_proba sums to 1 and non-negative\n",
    "X_prob = np.random.randn(40, 3)\n",
    "y_prob = np.random.randint(0, 3, 40)\n",
    "prob_clf = DecisionTreeClassifier(random_state=0).fit(X_prob, y_prob)\n",
    "probas = prob_clf.predict_proba(X_prob)\n",
    "check_vals(probas.sum(axis=1), np.ones(probas.shape[0]))\n",
    "assert np.all((probas >= 0) & (probas <= 1))\n",
    "\n",
    "# Test 9: score equals manual accuracy\n",
    "X_sc = np.random.randn(80, 4)\n",
    "y_sc = np.random.randint(0, 2, 80)\n",
    "sc_clf = DecisionTreeClassifier(max_depth=4, random_state=0).fit(X_sc, y_sc)\n",
    "acc_manual = np.mean(sc_clf.predict(X_sc) == y_sc)\n",
    "check_vals(sc_clf.score(X_sc, y_sc), acc_manual)\n",
    "\n",
    "# Test 10: Single-class dataset -> depth 0, one leaf, constant preds\n",
    "X_one = np.random.randn(25, 3)\n",
    "y_one = np.ones(25, dtype=int)\n",
    "one_clf = DecisionTreeClassifier().fit(X_one, y_one)\n",
    "assert one_clf.get_depth() == 0\n",
    "assert one_clf.get_n_leaves() == 1\n",
    "check_vals(one_clf.predict(X_one), y_one)\n",
    "\n",
    "# Test 11: Sklearn parity on simple synthetic data\n",
    "X_syn = np.random.randn(50, 3)\n",
    "y_syn = (X_syn[:, 0] + X_syn[:, 1] > 0).astype(int)\n",
    "ours_syn = DecisionTreeClassifier(max_depth=3, min_samples_split=5, random_state=0).fit(X_syn, y_syn)\n",
    "sk_syn = SklearnDTC(max_depth=3, min_samples_split=5, random_state=0).fit(X_syn, y_syn)\n",
    "check_vals(ours_syn.score(X_syn, y_syn), sk_syn.score(X_syn, y_syn), rtol=0.15)\n",
    "\n",
    "# Test 12: Sklearn parity on Iris\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "ours_iris = DecisionTreeClassifier(max_depth=4, random_state=0).fit(X_iris, y_iris)\n",
    "sk_iris = SklearnDTC(max_depth=4, random_state=0).fit(X_iris, y_iris)\n",
    "check_vals(ours_iris.score(X_iris, y_iris), sk_iris.score(X_iris, y_iris), rtol=0.10)\n",
    "\n",
    "# Test 13: Exact reproduction on controlled toy data\n",
    "X_exact = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "y_exact = np.array([0, 0, 0, 1, 1, 1])\n",
    "ours_exact = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X_exact, y_exact)\n",
    "sk_exact = SklearnDTC(max_depth=2, random_state=0).fit(X_exact, y_exact)\n",
    "assert np.array_equal(ours_exact.predict(X_exact), sk_exact.predict(X_exact))\n",
    "\n",
    "# Test 14: loss equals misclassification rate\n",
    "X_loss = np.array([[0], [1], [2], [3]])\n",
    "y_loss = np.array([0, 0, 1, 1])\n",
    "loss_clf = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X_loss, y_loss)\n",
    "assert loss_clf.loss(X_loss, y_loss) == approx(0.0, abs=1e-6)\n",
    "\n",
    "print(\"unit tests completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cb8e3",
   "metadata": {},
   "source": [
    "## Part 4. Main\n",
    "\n",
    "End-to-end experiment on the Breast Cancer Wisconsin dataset (Dua and Graff, 2019; Street et al., 1993): load the provided train/validation splits, fit our CART with sensible depth/splitting constraints, compare side-by-side with `sklearn.tree.DecisionTreeClassifier`, report accuracy and a confusion matrix, and sweep `max_depth` to visualize the bias–variance trade-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f00c5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main - Run CART on Breast Cancer Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDTC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "figures_dir = '../figures'\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "# Breast Cancer Wisconsin feature names\n",
    "FEATURE_NAMES = [\n",
    "    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "    'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "    'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
    "    'radius error', 'texture error', 'perimeter error', 'area error',\n",
    "    'smoothness error', 'compactness error', 'concavity error',\n",
    "    'concave points error', 'symmetry error', 'fractal dimension error',\n",
    "    'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
    "    'worst smoothness', 'worst compactness', 'worst concavity',\n",
    "    'worst concave points', 'worst symmetry', 'worst fractal dimension'\n",
    "]\n",
    "\n",
    "def load_breast_cancer_data():\n",
    "    \"\"\"\n",
    "    Load and prepare the Breast Cancer Wisconsin dataset.\n",
    "    \n",
    "    Reads pre-split training and validation data from CSV files in the ../data/ directory\n",
    "    and converts them to numpy arrays with appropriate data types.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train : ndarray of shape (n_train_samples, n_features)\n",
    "        Training feature matrix.\n",
    "    X_test : ndarray of shape (n_val_samples, n_features)\n",
    "        Validation feature matrix.\n",
    "    Y_train : ndarray of shape (n_train_samples,)\n",
    "        Training labels (integers).\n",
    "    Y_test : ndarray of shape (n_val_samples,)\n",
    "        Validation labels (integers).\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Expected file structure:\n",
    "        ../data/X_train.csv\n",
    "        ../data/y_train.csv\n",
    "        ../data/X_test.csv\n",
    "        ../data/y_test.csv\n",
    "    \"\"\"\n",
    "    X_train = pd.read_csv('../data/X_train.csv', header=None)\n",
    "    Y_train = pd.read_csv('../data/y_train.csv', header=None)\n",
    "    X_test = pd.read_csv('../data/X_test.csv', header=None)\n",
    "    Y_test = pd.read_csv('../data/y_test.csv', header=None)\n",
    "    \n",
    "    Y_train = np.array([i[0] for i in Y_train.values], dtype=int)\n",
    "    Y_test = np.array([i[0] for i in Y_test.values], dtype=int)\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    print(\"Loaded data from data/ directory\")\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "# Main - Run CART on Breast Cancer Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDTC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "figures_dir = '../figures'\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "# Breast Cancer Wisconsin feature names\n",
    "FEATURE_NAMES = [\n",
    "    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "    'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "    'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
    "    'radius error', 'texture error', 'perimeter error', 'area error',\n",
    "    'smoothness error', 'compactness error', 'concavity error',\n",
    "    'concave points error', 'symmetry error', 'fractal dimension error',\n",
    "    'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
    "    'worst smoothness', 'worst compactness', 'worst concavity',\n",
    "    'worst concave points', 'worst symmetry', 'worst fractal dimension'\n",
    "]\n",
    "\n",
    "def load_breast_cancer_data():\n",
    "    \"\"\"\n",
    "    Load and prepare the Breast Cancer Wisconsin dataset.\n",
    "    \n",
    "    Reads pre-split training and validation data from CSV files in the ../data/ directory\n",
    "    and converts them to numpy arrays with appropriate data types.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train : ndarray of shape (n_train_samples, n_features)\n",
    "        Training feature matrix.\n",
    "    X_test : ndarray of shape (n_val_samples, n_features)\n",
    "        Validation feature matrix.\n",
    "    Y_train : ndarray of shape (n_train_samples,)\n",
    "        Training labels (integers).\n",
    "    Y_test : ndarray of shape (n_val_samples,)\n",
    "        Validation labels (integers).\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Expected file structure:\n",
    "        ../data/X_train.csv\n",
    "        ../data/y_train.csv\n",
    "        ../data/X_test.csv\n",
    "        ../data/y_test.csv\n",
    "    \"\"\"\n",
    "    X_train = pd.read_csv('../data/X_train.csv', header=None)\n",
    "    Y_train = pd.read_csv('../data/y_train.csv', header=None)\n",
    "    X_test = pd.read_csv('../data/X_test.csv', header=None)\n",
    "    Y_test = pd.read_csv('../data/y_test.csv', header=None)\n",
    "    \n",
    "    Y_train = np.array([i[0] for i in Y_train.values], dtype=int)\n",
    "    Y_test = np.array([i[0] for i in Y_test.values], dtype=int)\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    print(\"Loaded data from data/ directory\")\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "185ca7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for CART decision tree experiment.\n",
    "    \n",
    "    This function performs a comprehensive evaluation of the CART implementation:\n",
    "    1. Loads the Breast Cancer Wisconsin dataset\n",
    "    2. Performs hyperparameter tuning to find optimal tree depth\n",
    "    3. Trains our CART implementation and sklearn's implementation with optimal parameters\n",
    "    4. Compares performance metrics between both implementations\n",
    "    5. Generates detailed classification reports and confusion matrices\n",
    "    6. Analyzes feature importance\n",
    "    7. Creates and saves visualization plots\n",
    "    \n",
    "    The experiment demonstrates that our implementation achieves comparable\n",
    "    performance to sklearn's production-grade implementation.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CART DECISION TREE CLASSIFIER - BREAST CANCER CLASSIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n### Loading Data ###\")\n",
    "    X_train, X_test, Y_train, Y_test = load_breast_cancer_data()\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "    print(f\"Class distribution in training: {np.bincount(Y_train)}\")\n",
    "    \n",
    "    # ===== STEP 1: Hyperparameter Tuning =====\n",
    "    print(\"\\n### Hyperparameter Tuning & Bias-Variance Trade-off ###\")\n",
    "\n",
    "    depths = [2, 3, 4, 5, 6, 7, 8]\n",
    "    min_samples_split_values = [2, 5]\n",
    "    min_samples_leaf_values = [1, 2, 5]\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    # Manual 3-fold CV over hyperparameters on training data\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    best_val_acc = -np.inf\n",
    "    best_depth = None\n",
    "    best_min_samples_split = None\n",
    "    best_min_samples_leaf = None\n",
    "\n",
    "    for depth in depths:\n",
    "        best_depth_train = -np.inf\n",
    "        best_depth_val = -np.inf\n",
    "        best_depth_split = None\n",
    "        best_depth_leaf = None\n",
    "\n",
    "        for min_split in min_samples_split_values:\n",
    "            for min_leaf in min_samples_leaf_values:\n",
    "                fold_train_scores = []\n",
    "                fold_val_scores = []\n",
    "\n",
    "                for train_idx, val_idx in skf.split(X_train, Y_train):\n",
    "                    X_tr, X_va = X_train[train_idx], X_train[val_idx]\n",
    "                    y_tr, y_va = Y_train[train_idx], Y_train[val_idx]\n",
    "\n",
    "                    clf_temp = DecisionTreeClassifier(\n",
    "                        max_depth=depth,\n",
    "                        min_samples_split=min_split,\n",
    "                        min_samples_leaf=min_leaf,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    clf_temp.fit(X_tr, y_tr)\n",
    "                    fold_train_scores.append(clf_temp.score(X_tr, y_tr))\n",
    "                    fold_val_scores.append(clf_temp.score(X_va, y_va))\n",
    "\n",
    "                mean_train = float(np.mean(fold_train_scores))\n",
    "                mean_val = float(np.mean(fold_val_scores))\n",
    "\n",
    "                if mean_val > best_depth_val:\n",
    "                    best_depth_val = mean_val\n",
    "                    best_depth_train = mean_train\n",
    "                    best_depth_split = min_split\n",
    "                    best_depth_leaf = min_leaf\n",
    "\n",
    "                if mean_val > best_val_acc:\n",
    "                    best_val_acc = mean_val\n",
    "                    best_depth = depth\n",
    "                    best_min_samples_split = min_split\n",
    "                    best_min_samples_leaf = min_leaf\n",
    "\n",
    "        train_accs.append(best_depth_train)\n",
    "        val_accs.append(best_depth_val)\n",
    "        print(\n",
    "            f\"  max_depth={depth:2d} best combo -> split={best_depth_split}, \"\n",
    "            f\"leaf={best_depth_leaf}, Val Acc={best_depth_val:.4f}, \"\n",
    "            f\"Train Acc (CV mean)={best_depth_train:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Selected params -> max_depth={best_depth}, min_samples_split={best_min_samples_split}, \"\n",
    "        f\"min_samples_leaf={best_min_samples_leaf} (Mean CV Acc: {best_val_acc:.4f})\"\n",
    "    )\n",
    "\n",
    "# ===== STEP 2: Train Final Model with Optimal Depth =====\n",
    "    print(\"\\n### Training Our CART Implementation with Optimal Depth ###\")\n",
    "    our_clf = DecisionTreeClassifier(\n",
    "        max_depth=best_depth,\n",
    "        min_samples_split=best_min_samples_split,\n",
    "        min_samples_leaf=best_min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    our_clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = our_clf.predict(X_train)\n",
    "    test_pred = our_clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = our_clf.score(X_train, Y_train)\n",
    "    test_acc = our_clf.score(X_test, Y_test)\n",
    "    \n",
    "    print(f\"\\nOur Implementation Results (max_depth={best_depth}, min_samples_split={best_min_samples_split}, min_samples_leaf={best_min_samples_leaf}):\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Tree Depth: {our_clf.get_depth()}\")\n",
    "    print(f\"  Number of Leaves: {our_clf.get_n_leaves()}\")\n",
    "\n",
    "    # Train sklearn's implementation for comparison\n",
    "    print(\"\\n### Training sklearn's Implementation ###\")\n",
    "    sk_clf = SklearnDTC(\n",
    "        max_depth=best_depth,\n",
    "        min_samples_split=best_min_samples_split,\n",
    "        min_samples_leaf=best_min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    sk_clf.fit(X_train, Y_train)\n",
    "    \n",
    "    sk_test_pred = sk_clf.predict(X_test)\n",
    "    sk_train_acc = sk_clf.score(X_train, Y_train)\n",
    "    sk_test_acc = sk_clf.score(X_test, Y_test)\n",
    "    sk_cm = confusion_matrix(Y_test, sk_test_pred)\n",
    "    \n",
    "    print(f\"\\nsklearn Implementation Results (max_depth={best_depth}, min_samples_split={best_min_samples_split}, min_samples_leaf={best_min_samples_leaf}):\")\n",
    "    print(f\"  Training Accuracy: {sk_train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy: {sk_test_acc:.4f}\")\n",
    "    print(f\"  Tree Depth: {sk_clf.get_depth()}\")\n",
    "    print(f\"  Number of Leaves: {sk_clf.get_n_leaves()}\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n### Comparison ###\")\n",
    "    print(f\"Training Accuracy Difference: {abs(train_acc - sk_train_acc):.6f}\")\n",
    "    print(f\"Test Accuracy Difference: {abs(test_acc - sk_test_acc):.6f}\")\n",
    "    print(f\"Tree Depth Difference: {abs(our_clf.get_depth() - sk_clf.get_depth())}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\n### Detailed Classification Report (Our Implementation) ###\")\n",
    "    print(classification_report(Y_test, test_pred, target_names=['Malignant', 'Benign']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\n### Confusion Matrix (Our Implementation) ###\")\n",
    "    cm = confusion_matrix(Y_test, test_pred)\n",
    "    print(cm)\n",
    "    print(\"\\nConfusion Matrix Interpretation:\")\n",
    "    print(f\"  True Negatives (Malignant correctly classified): {cm[0, 0]}\")\n",
    "    print(f\"  False Positives (Malignant misclassified as Benign): {cm[0, 1]}\")\n",
    "    print(f\"  False Negatives (Benign misclassified as Malignant): {cm[1, 0]}\")\n",
    "    print(f\"  True Positives (Benign correctly classified): {cm[1, 1]}\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    print(\"\\n### Feature Importance Analysis ###\")\n",
    "    importances = our_clf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    print(\"Top 5 Features (by Gini Importance):\")\n",
    "    for f in range(min(5, X_train.shape[1])):\n",
    "        print(f\"  {FEATURE_NAMES[indices[f]]}: {importances[indices[f]]:.4f}\")\n",
    "\n",
    "\n",
    "# ===== STEP 3: Generate Plots =====\n",
    "    # Plot 1: Bias-Variance Trade-off\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(depths, train_accs, 'o-', label='Training Accuracy', linewidth=2, markersize=8)\n",
    "    plt.plot(depths, val_accs, 's-', label='Validation Accuracy', linewidth=2, markersize=8)\n",
    "    plt.axvline(x=best_depth, color='red', linestyle='--', alpha=0.7, label=f'Optimal Depth={best_depth}')\n",
    "    plt.xlabel('Maximum Tree Depth', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Bias-Variance Trade-off', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    bias_variance_path = os.path.join(figures_dir, 'bias_variance_tradeoff.png')\n",
    "    plt.savefig(bias_variance_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nSaved figure: {bias_variance_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Feature Importances (Top 5)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_k = 5\n",
    "    top_indices = indices[:top_k]\n",
    "    top_importances = importances[top_indices]\n",
    "    top_names = [FEATURE_NAMES[i] for i in top_indices]\n",
    "    \n",
    "    y_pos = np.arange(top_k)\n",
    "    plt.barh(y_pos, top_importances, align='center')\n",
    "    plt.yticks(y_pos, top_names, fontsize=10)\n",
    "    plt.xlabel('Normalized Importance', fontsize=12)\n",
    "    plt.title(f'Top {top_k} Feature Importances', fontsize=14)\n",
    "    plt.gca().invert_yaxis()  # Highest importance at top\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    feature_importance_path = os.path.join(figures_dir, 'feature_importance.png')\n",
    "    plt.savefig(feature_importance_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved figure: {feature_importance_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 3: Confusion matrix visualization (Our CART)\n",
    "    labels = ['Malignant', 'Benign']\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    ax.set_title('Confusion Matrix - Our CART', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    cm_path = os.path.join(figures_dir, 'confusion_matrix_our_cart.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved figure: {cm_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 4: Confusion matrix visualization (sklearn CART)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sk_disp = ConfusionMatrixDisplay(confusion_matrix=sk_cm, display_labels=labels)\n",
    "    sk_disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    ax.set_title('Confusion Matrix - sklearn CART', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    sk_cm_path = os.path.join(figures_dir, 'confusion_matrix_sklearn_cart.png')\n",
    "    plt.savefig(sk_cm_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved figure: {sk_cm_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 5: ROC curve comparison\n",
    "    our_test_proba = our_clf.predict_proba(X_test)[:, 1]\n",
    "    sk_test_proba = sk_clf.predict_proba(X_test)[:, 1]\n",
    "    fpr_ours, tpr_ours, _ = roc_curve(Y_test, our_test_proba, pos_label=1)\n",
    "    fpr_sk, tpr_sk, _ = roc_curve(Y_test, sk_test_proba, pos_label=1)\n",
    "    auc_ours = auc(fpr_ours, tpr_ours)\n",
    "    auc_sk = auc(fpr_sk, tpr_sk)\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(fpr_ours, tpr_ours, label=f'Our CART (AUC={auc_ours:.3f})', linewidth=2)\n",
    "    plt.plot(fpr_sk, tpr_sk, label=f'sklearn CART (AUC={auc_sk:.3f})', linewidth=2, linestyle='--')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.4, linewidth=1)\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curve (Validation)', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    roc_path = os.path.join(figures_dir, 'roc_curve.png')\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved figure: {roc_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 6: Precision-Recall curve comparison\n",
    "    prec_ours, rec_ours, _ = precision_recall_curve(Y_test, our_test_proba, pos_label=1)\n",
    "    prec_sk, rec_sk, _ = precision_recall_curve(Y_test, sk_test_proba, pos_label=1)\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(rec_ours, prec_ours, label='Our CART', linewidth=2)\n",
    "    plt.plot(rec_sk, prec_sk, label='sklearn CART', linewidth=2, linestyle='--')\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve (Validation)', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    pr_path = os.path.join(figures_dir, 'precision_recall_curve.png')\n",
    "    plt.savefig(pr_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved figure: {pr_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nValidation accuracies by depth: {list(zip(depths, val_accs))}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3775fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CART DECISION TREE CLASSIFIER - BREAST CANCER CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      "### Loading Data ###\n",
      "Loaded data from data/ directory\n",
      "Training set size: 455 samples, 30 features\n",
      "Test set size: 114 samples\n",
      "Class distribution in training: [171 284]\n",
      "\n",
      "### Hyperparameter Tuning & Bias-Variance Trade-off ###\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "np.random.seed(42)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b79738",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J. (1984) *Classification and Regression Trees*. Wadsworth, Belmont, CA.\n",
    "- Loh, W.Y. (2011) 'Classification and regression trees', *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, 1(1), pp. 14–23.\n",
    "- Pedregosa, F. *et al.* (2011) 'Scikit-learn: Machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830.\n",
    "- Dua, D. and Graff, C. (2019) 'UCI Machine Learning Repository'. University of California, Irvine, School of Information and Computer Science. Available at: https://archive.ics.uci.edu/ml (Accessed: 6 December 2025).\n",
    "- Street, W.N., Wolberg, W.H. and Mangasarian, O.L. (1993) 'Nuclear feature extraction for breast tumor diagnosis', *Biomedical Image Processing and Biomedical Visualization*, pp. 861–870.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
